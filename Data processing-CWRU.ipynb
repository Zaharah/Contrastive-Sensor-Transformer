{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "515a0249",
   "metadata": {},
   "source": [
    "The dataset is obtained from https://engineering.case.edu/bearingdatacenter/download-data-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa4f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1218a0a",
   "metadata": {},
   "source": [
    "#### Data loader functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d23068",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Preprocessing snippet from https://github.com/XiongMeijing/CWRU-1/blob/master/helper.py'''\n",
    "\n",
    "def matfile_to_dic(folder_path):\n",
    "    '''\n",
    "    Read all the matlab files of the CWRU Bearing Dataset and return a \n",
    "    dictionary. The key of each item is the filename and the value is the data \n",
    "    of one matlab file, which also has key value pairs.\n",
    "    \n",
    "    Parameter:\n",
    "        folder_path: \n",
    "            Path (Path object) of the folder which contains the matlab files.\n",
    "    Return:\n",
    "        output_dic: \n",
    "            Dictionary which contains data of all files in the folder_path.\n",
    "    '''\n",
    "    output_dic = {}\n",
    "    for _, filepath in enumerate(folder_path.glob('*.mat')):\n",
    "        # strip the folder path and get the filename only.\n",
    "        key_name = str(filepath).split('\\\\')[-1]\n",
    "        output_dic[key_name] = io.loadmat(filepath)\n",
    "    return output_dic\n",
    "\n",
    "def remove_dic_items(dic):\n",
    "    '''\n",
    "    Remove redundant data in the dictionary returned by matfile_to_dic inplace.\n",
    "    '''\n",
    "    # For each file in the dictionary, delete the redundant key-value pairs\n",
    "    for _, values in dic.items():\n",
    "        del values['__header__']\n",
    "        del values['__version__']    \n",
    "        del values['__globals__']\n",
    "\n",
    "def rename_keys(dic):\n",
    "    '''\n",
    "    Rename some keys so that they can be loaded into a \n",
    "    DataFrame with consistent column names\n",
    "    '''\n",
    "    # For each file in the dictionary\n",
    "    for _,v1 in dic.items():\n",
    "        # For each key-value pair, rename the following keys \n",
    "        for k2,_ in list(v1.items()):\n",
    "            if 'DE_time' in k2:\n",
    "                v1['DE_time'] = v1.pop(k2)\n",
    "            elif 'BA_time' in k2:\n",
    "                v1['BA_time'] = v1.pop(k2)\n",
    "            elif 'FE_time' in k2:\n",
    "                v1['FE_time'] = v1.pop(k2)\n",
    "            elif 'RPM' in k2:\n",
    "                v1['RPM'] = v1.pop(k2)\n",
    "                \n",
    "def label(filename):\n",
    "    '''\n",
    "    Function to create label for each signal based on the filename. Apply this\n",
    "    to the \"filename\" column of the DataFrame.\n",
    "    Usage:\n",
    "        df['label'] = df['filename'].apply(label)\n",
    "    '''\n",
    "    if 'B007' in filename:\n",
    "        return 1\n",
    "    elif 'IR007' in filename:\n",
    "        return 2\n",
    "    elif 'OR007' in filename:\n",
    "        return 3\n",
    "    elif 'B014' in filename:\n",
    "        return 4\n",
    "    elif 'IR014' in filename:\n",
    "        return 5\n",
    "    elif 'OR014' in filename:\n",
    "        return 6\n",
    "    elif 'B021' in filename:\n",
    "        return 7\n",
    "    elif 'IR021' in filename:\n",
    "        return 8\n",
    "    elif 'OR021' in filename:\n",
    "        return 9\n",
    "    elif 'normal' in filename:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def fault_severity(filename):\n",
    "    '''\n",
    "    Function to create label for each signal based on the filename. Apply this\n",
    "    to the \"filename\" column of the DataFrame.\n",
    "    Usage:\n",
    "        df['fault_severity'] = df['filename'].apply(fault_severity)\n",
    "    '''\n",
    "    if '007' in filename:\n",
    "        return 7\n",
    "    elif '014' in filename:\n",
    "        return 14\n",
    "    elif '021' in filename:\n",
    "        return 21\n",
    "    elif 'normal' in filename:\n",
    "        return 0\n",
    "    \n",
    "def fault_type(filename):\n",
    "    '''\n",
    "    Function to create label for each signal based on the filename. Apply this\n",
    "    to the \"filename\" column of the DataFrame.\n",
    "    Usage:\n",
    "        df['fault_type'] = df['filename'].apply(fault_type)\n",
    "    '''\n",
    "    if 'B' in filename:\n",
    "        return 'B'\n",
    "    elif 'IR' in filename:\n",
    "        return 'IR'\n",
    "    elif 'OR' in filename:\n",
    "        return 'OR'\n",
    "    elif 'normal' in filename:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def divide_signal(df, segment_length):\n",
    "    '''\n",
    "    This function divide the signal into segments, each with a specific number \n",
    "    of points as defined by segment_length. Each segment will be added as an \n",
    "    example (a row) in the returned DataFrame. Thus it increases the number of \n",
    "    training examples. The remaining points which are less than segment_length \n",
    "    are discarded.\n",
    "    \n",
    "    Parameter:\n",
    "        df: \n",
    "            DataFrame returned by matfile_to_df()\n",
    "        segment_length: \n",
    "            Number of points per segment.\n",
    "    Return:\n",
    "        DataFrame with segmented signals and their corresponding filename and \n",
    "        label\n",
    "    '''\n",
    "    dic = {}\n",
    "    idx = 0\n",
    "    for i in range(df.shape[0]):\n",
    "        n_sample_points = len(df.iloc[i,1])\n",
    "        n_segments = n_sample_points // segment_length\n",
    "        for segment in range(n_segments):\n",
    "            dic[idx] = {\n",
    "                'signal': df.iloc[i,1][segment_length * segment:segment_length * (segment+1)], \n",
    "                'label': df.iloc[i,2],\n",
    "                'filename' : df.iloc[i,0]\n",
    "            }\n",
    "            idx += 1\n",
    "    df_tmp = pd.DataFrame.from_dict(dic,orient='index')\n",
    "    df_output = pd.concat(\n",
    "        [df_tmp[['label', 'filename']], \n",
    "         pd.DataFrame(np.hstack(df_tmp[\"signal\"].values).T)\n",
    "        ], \n",
    "        axis=1 )\n",
    "    return df_output\n",
    "\n",
    "\n",
    "def normalize_signal(df):\n",
    "    '''\n",
    "    Normalize the signals in the DataFrame returned by matfile_to_df() by subtracting\n",
    "    the mean and dividing by the standard deviation.\n",
    "    '''\n",
    "    mean = df['DE_time'].apply(np.mean)\n",
    "    std = df['DE_time'].apply(np.std)\n",
    "    df['DE_time'] = (df['DE_time'] - mean) / std\n",
    "    \n",
    "    mean = df['FE_time'].apply(np.mean)\n",
    "    std = df['FE_time'].apply(np.std)\n",
    "    df['FE_time'] = (df['FE_time'] - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841bfc4",
   "metadata": {},
   "source": [
    "#### Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd5c8fc",
   "metadata": {},
   "source": [
    "Data classes:\n",
    "    \n",
    "    - 0: Normal\n",
    "    - 1: B007\n",
    "    - 2: IR007\n",
    "    - 3: OR007\n",
    "    - 4: B014\n",
    "    - 5: IR014\n",
    "    - 6: OR014\n",
    "    - 7: B021\n",
    "    - 8: IR021\n",
    "    - 9: OR021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506ce88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('Data/48kDE_CWRU/')\n",
    "dic = matfile_to_dic(data_path)\n",
    "remove_dic_items(dic)\n",
    "rename_keys(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dff01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(dic).T\n",
    "df = df.reset_index().rename(mapper={'index':'filename'},axis=1)\n",
    "df['label'] = df['filename'].apply(label)\n",
    "df['operating_condition'] = df['filename'].apply(operating_condition)\n",
    "df['fault_severity'] = df['filename'].apply(fault_severity)\n",
    "df['fault_type'] = df['filename'].apply(fault_type)\n",
    "df.drop(['X217','RPM', 'ans'], axis=1, errors='ignore', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2243a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_signal(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc0be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_de = df.drop(['BA_time','FE_time', 'RPM', 'ans'], axis=1, errors='ignore')\n",
    "df_de_pr = divide_signal(df_de, 512)\n",
    "df_de_pr['label'] = df_de_pr['filename'].apply(label)\n",
    "df_fe = df.drop(['BA_time','DE_time', 'RPM', 'ans'], axis=1, errors='ignore')\n",
    "df_fe_pr = divide_signal(df_fe, 512)\n",
    "df_fe_pr['label'] = df_fe_pr['filename'].apply(label)\n",
    "df_de_arr = np.array(df_de_pr[df_de_pr.columns[2:]])\n",
    "df_fe_arr = np.array(df_fe_pr[df_fe_pr.columns[2:]])\n",
    "df_both = np.dstack((df_fe_arr, df_de_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee10baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data into train and validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_both, \n",
    "                                                      df_de_pr['label'].values,\n",
    "                                                      test_size=0.30, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1805a1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbc54ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Data/48kDE_CWRU_processed/normalized_train_test/X_train.npy', X_train)\n",
    "np.save('Data/48kDE_CWRU_processed/normalized_train_test/X_test.npy', X_test)\n",
    "np.save('Data/48kDE_CWRU_processed/normalized_train_test/y_train.npy', y_train)\n",
    "np.save('Data/48kDE_CWRU_processed/normalized_train_test/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aebb149",
   "metadata": {},
   "source": [
    "#### Fraction of samples for data efficient experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18834291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.load('Data/48kDE_CWRU_processed/train_test/X_train.npy')\n",
    "# y_train = np.load('Data/48kDE_CWRU_processed/train_test/y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b1eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_by_number_sample(X_train, y_train):\n",
    "    number_samples = [5, 10, 20, 50, 100]\n",
    "    for samples in number_samples:\n",
    "        indexes = []\n",
    "        for label in range(len(np.unique(y_train))):\n",
    "            ind = np.where(y_train == label)[0]\n",
    "            selected_ind = np.random.choice(ind, samples)\n",
    "            indexes.append(selected_ind)\n",
    "            print(samples,label,  X_train[selected_ind].shape)\n",
    "        s_indexes = np.hstack(indexes)\n",
    "        print(s_indexes, len(s_indexes))\n",
    "        features = X_train[s_indexes]\n",
    "        labels = y_train[s_indexes] \n",
    "        np.save('Data/48kDE_CWRU_processed/normalized_number_samples/'+str(samples)+'_X_train.npy', features)\n",
    "        np.save('Data/48kDE_CWRU_processed/normalized_number_samples/'+str(samples)+'_y_train.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaec713",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_by_number_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513e8eff",
   "metadata": {},
   "source": [
    "#### Invariance to noval fault types and severity (clustering)\n",
    "- Train on all dataset except for fault type B [1,4,7] with all fault severities \n",
    "- Train on all dataset except of fault type IR [2,5,8] with all fault severities\n",
    "- Train on all datset except for fault type OR [3,6,9] with all fault severities. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('Data/48kDE_CWRU_processed/train_test/X_train.npy')\n",
    "y_train = np.load('Data/48kDE_CWRU_processed/train_test/y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755bc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_fault_types(x_train, y_train, fault_remove_list, fault_category):\n",
    "    indexes = []\n",
    "    for fault in fault_remove_list:\n",
    "        indexes.append(np.where(y_train==fault)[0])\n",
    "    s_indexes = np.hstack(indexes)\n",
    "    print(s_indexes.shape)\n",
    "    X_train_sample = np.delete(x_train, s_indexes, axis=0)\n",
    "    y_train_sample = np.delete(y_train, s_indexes, axis=0)\n",
    "    print(X_train_sample.shape, y_train_sample.shape)\n",
    "    np.save('Data/48kDE_CWRU_processed/fault_types/'+fault_category+'/X_train.npy', X_train_sample)\n",
    "    np.save('Data/48kDE_CWRU_processed/fault_types/'+fault_category+'/y_train.npy', y_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_fault_types(x_train, y_train, [1,4,7], 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_fault_types(x_train, y_train, [2,5,8], 'IR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b47ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_fault_types(x_train, y_train, [3,6,9], 'OR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e831b8",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
