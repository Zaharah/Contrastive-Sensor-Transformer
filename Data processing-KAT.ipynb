{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f10a179",
   "metadata": {},
   "source": [
    "The datatset is obtained from https://mb.uni-paderborn.de/en/kat/main-research/datacenter/bearing-datacenter/data-sets-and-download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fa4f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac690055",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Data Loader functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0618d7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 0 for force 16005\n",
    "- 1 for phase_current_1 256070\n",
    "- 2 for phase_current_2 256070\n",
    "- 3 for speed 16005\n",
    "- 4 for temp_2_bearing_module 5\n",
    "- 5 for torque 16005\n",
    "- 6 for vibration signal 256070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37eae8a7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def matfile_to_array(folder_path, bearing_list, label, damage_type, damage_severity):\n",
    "    for bearing in bearing_list:\n",
    "        files_path = Path(folder_path + '/' + bearing)\n",
    "        operating_condition = ['N09_M07_F10', 'N15_M01_F10', 'N15_M07_F04', 'N15_M07_F10']\n",
    "        filesname_list = ['N09_M07_F10', 'N15_M01_F10', 'N15_M07_F04', 'N15_M07_F10']\n",
    "        for f in filesname_list:\n",
    "            f = f + '_'+bearing\n",
    "            filename = str(f).split('\\\\')[-1][:16]\n",
    "            for oc in operating_condition:\n",
    "                if oc == filename[:11]:\n",
    "                    data_readings = []\n",
    "                    for reading in range(1, 21):\n",
    "                        filename_reading = \"\".join([filename, '_', str(reading)])\n",
    "                        if filename_reading != \"N15_M01_F10_KA08_2\":\n",
    "                            print(\"\".join([folder_path, bearing, '/', filename_reading, '.mat']))\n",
    "                            file = io.loadmat(\"\".join([folder_path, bearing, '/', filename_reading, '.mat']))#'Data/PDUData/Healthy/K001/N09_M07_F10_K001_1.mat')\n",
    "                            data = file[filename_reading]['Y'][0][0][0][6][2][0]\n",
    "                            data_readings.append(data)\n",
    "                            #all_data = np.hstack(data_readings)\n",
    "                np.save(\"\".join([\"Data/KAT_preprocessing\", '/', bearing, '_', oc, '_', label, \n",
    "                                 '_', damage_severity, '_', damage_type, '.npy']),  np.array(data_readings).flatten())#all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ad66d4d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def array_to_dic(folder_path):\n",
    "    '''\n",
    "    Read all the matlab files of the CWRU Bearing Dataset and return a \n",
    "    dictionary. The key of each item is the filename and the value is the data \n",
    "    of one matlab file, which also has key value pairs.\n",
    "    \n",
    "    Parameter:\n",
    "        folder_path: \n",
    "            Path (Path object) of the folder which contains the matlab files.\n",
    "    Return:\n",
    "        output_dic: \n",
    "            Dictionary which contains data of all files in the folder_path.\n",
    "    '''\n",
    "    output_dic = {}\n",
    "    for _, filepath in enumerate(folder_path.glob('*.npy')):\n",
    "        # strip the folder path and get the filename only.\n",
    "        key_name = str(filepath).split('\\\\')[-1]\n",
    "        output_dic[key_name] = np.load(filepath, allow_pickle = True)\n",
    "    return output_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "608c1dcf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def label(filename):\n",
    "    '''\n",
    "    Function to create label for each signal based on the filename. Apply this\n",
    "    to the \"filename\" column of the DataFrame.\n",
    "    Usage:\n",
    "        df['label'] = df['filename'].apply(label)\n",
    "    '''\n",
    "    if 'H_S0' in filename:\n",
    "        return 0\n",
    "    elif 'IR_S1' in filename:\n",
    "        return 1\n",
    "    elif 'OR_S1' in filename:\n",
    "        return 2\n",
    "    elif 'IR_S2' in filename:\n",
    "        return 3\n",
    "    elif 'OR_S2' in filename:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57a446a5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def divide_signal(df, segment_length):\n",
    "    '''\n",
    "    This function divide the signal into segments, each with a specific number \n",
    "    of points as defined by segment_length. Each segment will be added as an \n",
    "    example (a row) in the returned DataFrame. Thus it increases the number of \n",
    "    training examples. The remaining points which are less than segment_length \n",
    "    are discarded.\n",
    "    \n",
    "    Parameter:\n",
    "        df: \n",
    "            DataFrame returned by matfile_to_df()\n",
    "        segment_length: \n",
    "            Number of points per segment.\n",
    "    Return:\n",
    "        DataFrame with segmented signals and their corresponding filename and \n",
    "        label\n",
    "    '''\n",
    "    dic = {}\n",
    "    idx = 0\n",
    "    for i in range(df.shape[0]):\n",
    "        n_sample_points = len(df.iloc[i,1])\n",
    "        n_segments = n_sample_points // segment_length\n",
    "        for segment in range(n_segments):\n",
    "            dic[idx] = {\n",
    "                'signal': df.iloc[i,1][segment_length * segment:segment_length * (segment+1)], \n",
    "                'label': df.iloc[i,2],\n",
    "                'filename' : df.iloc[i,0]\n",
    "            }\n",
    "            idx += 1\n",
    "    df_tmp = pd.DataFrame.from_dict(dic,orient='index')\n",
    "    df_output = pd.concat(\n",
    "        [df_tmp[['label', 'filename']], \n",
    "         pd.DataFrame(np.hstack(df_tmp[\"signal\"].values).T)\n",
    "        ], \n",
    "        axis=1 )\n",
    "    return df_tmp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a7de99",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0585eb7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "folder_path = 'Data/KAT/Healthy/'\n",
    "bearing_list = ['K001', 'K002', 'K003', 'K004', 'K005', 'K006']\n",
    "matfile_to_array(folder_path, bearing_list, 'H', 'None', 'S0')\n",
    "\n",
    "folder_path = 'Data/KAT/Real damage/'\n",
    "bearing_list = ['KA04', 'KA15', 'KA22', 'KA30']\n",
    "matfile_to_array(folder_path, bearing_list, 'OR', 'Real', 'S1')\n",
    "\n",
    "\n",
    "folder_path = 'Data/KAT/Real damage/'\n",
    "bearing_list = ['KA16']\n",
    "matfile_to_array(folder_path, bearing_list, 'OR', 'Real', 'S2')\n",
    "\n",
    "\n",
    "folder_path = 'Data/KAT/Real damage/'\n",
    "bearing_list = ['KI04', 'KI14', 'KI17', 'KI21']\n",
    "matfile_to_array(folder_path, bearing_list, 'IR', 'Real', 'S1')\n",
    "\n",
    "\n",
    "folder_path = 'Data/KAT/Real damage/'\n",
    "bearing_list = ['KI18']\n",
    "matfile_to_array(folder_path, bearing_list, 'IR', 'Real', 'S2')\n",
    "\n",
    "\n",
    "folder_path = 'Data/KAT/Artifical damage/'\n",
    "bearing_list = ['KA01', 'KA05', 'KA07']#, 'KI01', 'KI03', 'KI05', 'KI07', 'KI08']\n",
    "matfile_to_array(folder_path, bearing_list, 'OR', 'Artifical', 'S1')\n",
    "\n",
    "folder_path = 'Data/KAT/Artifical damage/'\n",
    "bearing_list = ['KA03',  'KA06', 'KA08', 'KA09']#, 'KI01', 'KI03', 'KI05', 'KI07', 'KI08']\n",
    "matfile_to_array(folder_path, bearing_list, 'OR', 'Artifical', 'S2')\n",
    "\n",
    "folder_path = 'Data/KAT/Artifical damage/'\n",
    "bearing_list = ['KI01', 'KI03', 'KI05']\n",
    "matfile_to_array(folder_path, bearing_list, 'IR', 'Artifical', 'S1')\n",
    "\n",
    "folder_path = 'Data/KAT/Artifical damage/'\n",
    "bearing_list = ['KI07', 'KI08']\n",
    "matfile_to_array(folder_path, bearing_list, 'IR', 'Artifical', 'S2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e56d1b9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_path = Path('Data/KAT_preprocessing/')\n",
    "dic = array_to_dic(data_path)\n",
    "df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in dic.items()])).T\n",
    "df = df.reset_index().rename(mapper={'index':'filename'},axis=1)\n",
    "df['label'] = df['filename'].apply(label)\n",
    "# df.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4f5977",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "appended_data = []\n",
    "for i in range(20):\n",
    "    new_df = divide_signal(df[['filename', i, 'label']], 1200)\n",
    "    print(i, new_df.shape)\n",
    "    appended_data.append(new_df)\n",
    "df_all = pd.concat(appended_data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1c679bd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "signal_data = np.array(df_all[['signal']])\n",
    "out = np.vstack(signal_data[:,0])\n",
    "## Split the data into train and validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(out, \n",
    "                                                      df_all['label'],\n",
    "                                                      test_size=0.30, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74f6f162",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(335033, 1200) (143586, 1200) (335033,) (143586,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "135abc79",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b15e796",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(335033, 1200, 1) (143586, 1200, 1) (335033,) (143586,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08d4c8ed",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.save('Data/KAT_processed/train_test/X_train.npy', X_train)\n",
    "np.save('Data/KAT_processed/train_test/X_test.npy', X_test)\n",
    "np.save('Data/KAT_processed/train_test/y_train.npy', y_train)\n",
    "np.save('Data/KAT_processed/train_test/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303adad4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aecc0cc5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_all.to_csv('Data/KAT_processed/df_all_signals.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4404fb3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Fraction of samples for data efficient experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "780d6864",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load('Data/KAT_processed/train_test/X_train.npy')\n",
    "y_train = np.load('Data/KAT_processed/train_test/y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13db2eea",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def select_by_number_sample(X_train, y_train):\n",
    "    number_samples = [5, 10, 20, 50, 100]\n",
    "    for samples in number_samples:\n",
    "        indexes = []\n",
    "        for label in range(len(np.unique(y_train))):\n",
    "            ind = np.where(y_train == label)[0]\n",
    "            selected_ind = np.random.choice(ind, samples)\n",
    "            indexes.append(selected_ind)\n",
    "            print(samples,label,  X_train[selected_ind].shape)\n",
    "        s_indexes = np.hstack(indexes)\n",
    "        features = X_train[s_indexes]\n",
    "        print(features.shape)\n",
    "        labels = y_train[s_indexes] #to_categorical()\n",
    "        np.save('Data/KAT_processed/number_samples/'+str(samples)+'_X_train.npy', features)\n",
    "        np.save('Data/KAT_processed/number_samples/'+str(samples)+'_y_train.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6184faa",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0 (5, 1200, 1)\n",
      "5 1 (5, 1200, 1)\n",
      "5 2 (5, 1200, 1)\n",
      "5 3 (5, 1200, 1)\n",
      "5 4 (5, 1200, 1)\n",
      "(25, 1200, 1)\n",
      "10 0 (10, 1200, 1)\n",
      "10 1 (10, 1200, 1)\n",
      "10 2 (10, 1200, 1)\n",
      "10 3 (10, 1200, 1)\n",
      "10 4 (10, 1200, 1)\n",
      "(50, 1200, 1)\n",
      "20 0 (20, 1200, 1)\n",
      "20 1 (20, 1200, 1)\n",
      "20 2 (20, 1200, 1)\n",
      "20 3 (20, 1200, 1)\n",
      "20 4 (20, 1200, 1)\n",
      "(100, 1200, 1)\n",
      "50 0 (50, 1200, 1)\n",
      "50 1 (50, 1200, 1)\n",
      "50 2 (50, 1200, 1)\n",
      "50 3 (50, 1200, 1)\n",
      "50 4 (50, 1200, 1)\n",
      "(250, 1200, 1)\n",
      "100 0 (100, 1200, 1)\n",
      "100 1 (100, 1200, 1)\n",
      "100 2 (100, 1200, 1)\n",
      "100 3 (100, 1200, 1)\n",
      "100 4 (100, 1200, 1)\n",
      "(500, 1200, 1)\n"
     ]
    }
   ],
   "source": [
    "select_by_number_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc75259",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Invariance to noval fault types and severity (clustering)\n",
    "    - Train on all dataset except for fault type OR [2,4] with all damages severity \n",
    "    - Train on all dataset except for fault type IR [1,3] with all damages severity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01721fd8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_fault_types(x_train, y_train, fault_remove_list, fault_category):\n",
    "    indexes = []\n",
    "    for fault in fault_remove_list:\n",
    "        indexes.append(np.where(y_train==fault)[0])\n",
    "    s_indexes = np.hstack(indexes)\n",
    "    print(s_indexes.shape)\n",
    "    X_train_sample = np.delete(x_train, s_indexes, axis=0)\n",
    "    y_train_sample = np.delete(y_train, s_indexes, axis=0)\n",
    "    print(X_train_sample.shape, y_train_sample.shape)\n",
    "    np.save('Data/KAT_processed/fault_types/'+fault_category+'/X_train.npy', X_train_sample)\n",
    "    np.save('Data/KAT_processed/fault_types/'+fault_category+'/y_train.npy', y_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "745c1534",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_train = np.load('Data/KAT_processed/train_test/X_train.npy')\n",
    "y_train = np.load('Data/KAT_processed/train_test/y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27d95c40",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119734,)\n",
      "(215299, 1200, 1) (215299,)\n"
     ]
    }
   ],
   "source": [
    "remove_fault_types(x_train, y_train, [1,3], 'IR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "747e9740",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143639,)\n",
      "(191394, 1200, 1) (191394,)\n"
     ]
    }
   ],
   "source": [
    "remove_fault_types(x_train, y_train, [2,4], 'OR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5effd2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
